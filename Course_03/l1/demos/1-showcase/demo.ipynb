{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling credentials with .env files\n",
    "\n",
    "**NEVER** put your credentials in your code!\n",
    "\n",
    "An .env file is a simple text file that contains your credentials:\n",
    "\n",
    "```\n",
    "GEMINI_API_KEY=...\n",
    "OPENAI_API_KEY=...\n",
    "HUGGINFACE_API_KEY=...\n",
    "```\n",
    "\n",
    "Create it in your repository and **make sure to use a .gitignore file** with a line containing `.env`. This way it will be much more difficult to commit the .env file by mistake.\n",
    "\n",
    "A good practice is to provide a `env.example` file with some mock credentials, so whoever is going to use your code will just have to do:\n",
    "\n",
    "```bash\n",
    "cp env.example .env\n",
    "```\n",
    "and then edit the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials for Gemini\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Now all the variables in the .env file are available as:\n",
    "# os.environ.get(\"[variable name]\"), for example:\n",
    "# os.environ.get(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_image_from_url(image_url: str) -> Image.Image:\n",
    "    \n",
    "    return Image.open(BytesIO(requests.get(image_url).content)).copy()\n",
    "    \n",
    "\n",
    "def display_image_with_caption(image: Image.Image, caption: str):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(caption, wrap=True, fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# This URL generates a random JPG image every time you call it!\n",
    "image_url = \"https://picsum.photos/400/300\"\n",
    "image = get_image_from_url(image_url)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Describe this image briefly\"\n",
    "\n",
    "# The following is for huggingface\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "def analyze_image_with_gemini(img: Image.Image, prompt: str) -> str:\n",
    "    \n",
    "    client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "    config = types.GenerateContentConfig(\n",
    "        # Set thinking to 0 to save some money and get a faster response\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    )\n",
    "\n",
    "    # Limit size to at most 600x600 to limit the usage of tokens\n",
    "    img.thumbnail((600, 600))\n",
    "\n",
    "    # Format image for Gemini call\n",
    "    image_bytes = BytesIO()\n",
    "    img.save(image_bytes, format='JPEG')\n",
    "    image_part = types.Part.from_bytes(data=image_bytes.getvalue(), mime_type='image/jpeg')\n",
    "\n",
    "    # Call Gemini\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=[prompt, image_part],\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    return response.text\n",
    "    \n",
    "caption = analyze_image_with_gemini(\n",
    "    image,\n",
    "    prompt\n",
    ")\n",
    "\n",
    "display_image_with_caption(image, caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device is {device}\")\n",
    "\n",
    "model_path = \"HuggingFaceTB/SmolVLM2-500M-Video-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16, device_map=\"auto\",\n",
    ")\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device, dtype=torch.float16)\n",
    "\n",
    "# This takes around 10 seconds on GPU, around 4 minutes on a CPU\n",
    "generated_ids = model.generate(\n",
    "    **inputs, \n",
    "    do_sample=False, \n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,  # Ensure EOS token stops generation\n",
    "    pad_token_id=processor.tokenizer.eos_token_id\n",
    ")\n",
    "generated_texts = processor.batch_decode(\n",
    "    # We slice here to avoid decoding our own prompt, and instead\n",
    "    # only decode the output of the model\n",
    "    generated_ids[:, inputs['input_ids'].shape[1]:],\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "caption = generated_texts[0]\n",
    "\n",
    "display_image_with_caption(image, caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "os.environ['PATH'] += \":/workspace/ffmpeg-n7.1-latest-linux64-gpl-shared-7.1/bin\"\n",
    "os.environ['LD_LIBRARY_PATH'] = \"/workspace/ffmpeg-n7.1-latest-linux64-gpl-shared-7.1/lib\"\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# This takes a few seconds on a T4 GPU and around 1 m on CPU\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# NOTE: this requires ffmpeg to be installed in your system\n",
    "# This is the text spoken in this file:\n",
    "# Many animals of even complex structure which live parasitically within others are wholly devoid of an alimentary cavity.\n",
    "result = pipe(\"LJ025-0076.wav\")\n",
    "\n",
    "print(f\"Transcribed text:\\n\\n{result['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "36371cb60c26e37b7d9a2ceed614c6abe3cd2e9c2c4d621fd25f98fd923082ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
